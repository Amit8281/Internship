{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa6698c8",
   "metadata": {},
   "source": [
    "# Web Scraping Assignment-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99e6e5e",
   "metadata": {},
   "source": [
    "#### import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2a23bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b59149",
   "metadata": {},
   "source": [
    "## 1) Write a python program to display all the header tags from wikipedia.org and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f531093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function that takes in a URL as input\n",
    "def wikipedia_header(url):\n",
    "    \n",
    "    # Send a GET request to the Wikipedia page and parse its content using BeautifulSoup\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    # Initialize an empty list to store the header text\n",
    "    header = []\n",
    "\n",
    "    # Find the first span element with a class attribute of 'mw-headline' and append its text content to the header list\n",
    "    h1 = soup.find('span', class_='mw-headline')\n",
    "    if h1:\n",
    "        header.append(h1.text)\n",
    "\n",
    "    # Loop through all h2 elements with a class attribute of 'mp-h2' and append their text content to the header list   \n",
    "    for i in soup.find_all('h2', class_='mp-h2'):\n",
    "        header.append(i.text)\n",
    "\n",
    "    # Create a Pandas DataFrame from the header list   \n",
    "    df = pd.DataFrame({'HEADER': header})\n",
    "\n",
    "    # Set the DataFrame index to start from 1 instead of the default 0\n",
    "    df.index = range(1, len(df)+1)\n",
    "\n",
    "    # Return the DataFrame\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83d8d26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HEADER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Welcome to Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From today's featured article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Did you know ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>On this day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Today's featured picture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Other areas of Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wikipedia's sister projects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wikipedia languages</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          HEADER\n",
       "1           Welcome to Wikipedia\n",
       "2  From today's featured article\n",
       "3               Did you know ...\n",
       "4                    In the news\n",
       "5                    On this day\n",
       "6       Today's featured picture\n",
       "7       Other areas of Wikipedia\n",
       "8    Wikipedia's sister projects\n",
       "9            Wikipedia languages"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = wikipedia_header(\"https://en.wikipedia.org/wiki/Main_Page\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a205079d",
   "metadata": {},
   "source": [
    "## 2) Write a python program to display IMDB’s Top rated 50 movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99d64b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function that takes in a URL as input\n",
    "def imdb_top50(url):\n",
    "    \n",
    "\n",
    "    # Send a GET request to the IMDb Top 250 page and parse its content using BeautifulSoup\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    # Find all td elements with a class attribute of 'titleColumn' and extract the movie name, rating, and year of release for the first 50 movies\n",
    "    top_movies = soup.find_all('td', class_='titleColumn')\n",
    "    movie_name = []\n",
    "    movie_rating = []\n",
    "    year_release = []\n",
    "\n",
    "    for movie in top_movies[:50]:\n",
    "        # Extract the movie name from the <a> element inside the <td> element and append it to the movie_name list\n",
    "        name = movie.a.text\n",
    "        movie_name.append(name)\n",
    "\n",
    "        # Extract the movie rating from the <strong> element inside the <td> element's parent <td> element with a class attribute of 'ratingColumn imdbRating' and append it to the movie_rating list\n",
    "        rating = movie.parent.find(\"td\", class_=\"ratingColumn imdbRating\").strong.text\n",
    "        movie_rating.append(rating)\n",
    "\n",
    "        # Extract the year of release from the <span> element inside the <td> element and append it to the year_release list\n",
    "        year = movie.span.text.strip(\"()\")\n",
    "        year_release.append(year)\n",
    "\n",
    "    # Create a Pandas DataFrame from the extracted movie data  \n",
    "    df = pd.DataFrame({\n",
    "        'Name': movie_name,\n",
    "        'Rating': movie_rating,\n",
    "        'Year': year_release\n",
    "    })\n",
    "\n",
    "    # Set the DataFrame index to start from 1 instead of the default 0\n",
    "    df.index = range(1, len(df)+1)\n",
    "\n",
    "    # Return the DataFrame\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99956e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>9.2</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>9.2</td>\n",
       "      <td>1972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Godfather Part II</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Schindler's List</td>\n",
       "      <td>8.9</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Lord of the Rings: The Return of the King</td>\n",
       "      <td>8.9</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Pulp Fiction</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Lord of the Rings: The Fellowship of the Ring</td>\n",
       "      <td>8.8</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Il buono, il brutto, il cattivo</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Forrest Gump</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Fight Club</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The Lord of the Rings: The Two Towers</td>\n",
       "      <td>8.7</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Inception</td>\n",
       "      <td>8.7</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The Empire Strikes Back</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>The Matrix</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>GoodFellas</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>One Flew Over the Cuckoo's Nest</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Se7en</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>It's a Wonderful Life</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Shichinin no samurai</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>The Silence of the Lambs</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Saving Private Ryan</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Cidade de Deus</td>\n",
       "      <td>8.6</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Interstellar</td>\n",
       "      <td>8.6</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>La vita è bella</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>The Green Mile</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Star Wars</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Terminator 2: Judgment Day</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Back to the Future</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Sen to Chihiro no kamikakushi</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>The Pianist</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Psycho</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Gisaengchung</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Léon</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>The Lion King</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Gladiator</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>American History X</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>The Departed</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Whiplash</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>The Prestige</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>The Usual Suspects</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Casablanca</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Hotaru no haka</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Seppuku</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>The Intouchables</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Modern Times</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Once Upon a Time in the West</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Nuovo Cinema Paradiso</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Rear Window</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Name Rating  Year\n",
       "1                            The Shawshank Redemption    9.2  1994\n",
       "2                                       The Godfather    9.2  1972\n",
       "3                                     The Dark Knight    9.0  2008\n",
       "4                               The Godfather Part II    9.0  1974\n",
       "5                                        12 Angry Men    9.0  1957\n",
       "6                                    Schindler's List    8.9  1993\n",
       "7       The Lord of the Rings: The Return of the King    8.9  2003\n",
       "8                                        Pulp Fiction    8.8  1994\n",
       "9   The Lord of the Rings: The Fellowship of the Ring    8.8  2001\n",
       "10                    Il buono, il brutto, il cattivo    8.8  1966\n",
       "11                                       Forrest Gump    8.8  1994\n",
       "12                                         Fight Club    8.7  1999\n",
       "13              The Lord of the Rings: The Two Towers    8.7  2002\n",
       "14                                          Inception    8.7  2010\n",
       "15                            The Empire Strikes Back    8.7  1980\n",
       "16                                         The Matrix    8.7  1999\n",
       "17                                         GoodFellas    8.7  1990\n",
       "18                    One Flew Over the Cuckoo's Nest    8.6  1975\n",
       "19                                              Se7en    8.6  1995\n",
       "20                              It's a Wonderful Life    8.6  1946\n",
       "21                               Shichinin no samurai    8.6  1954\n",
       "22                           The Silence of the Lambs    8.6  1991\n",
       "23                                Saving Private Ryan    8.6  1998\n",
       "24                                     Cidade de Deus    8.6  2002\n",
       "25                                       Interstellar    8.6  2014\n",
       "26                                    La vita è bella    8.6  1997\n",
       "27                                     The Green Mile    8.6  1999\n",
       "28                                          Star Wars    8.5  1977\n",
       "29                         Terminator 2: Judgment Day    8.5  1991\n",
       "30                                 Back to the Future    8.5  1985\n",
       "31                      Sen to Chihiro no kamikakushi    8.5  2001\n",
       "32                                        The Pianist    8.5  2002\n",
       "33                                             Psycho    8.5  1960\n",
       "34                                       Gisaengchung    8.5  2019\n",
       "35                                               Léon    8.5  1994\n",
       "36                                      The Lion King    8.5  1994\n",
       "37                                          Gladiator    8.5  2000\n",
       "38                                 American History X    8.5  1998\n",
       "39                                       The Departed    8.5  2006\n",
       "40                                           Whiplash    8.5  2014\n",
       "41                                       The Prestige    8.5  2006\n",
       "42                                 The Usual Suspects    8.5  1995\n",
       "43                                         Casablanca    8.5  1942\n",
       "44                                     Hotaru no haka    8.5  1988\n",
       "45                                            Seppuku    8.5  1962\n",
       "46                                   The Intouchables    8.5  2011\n",
       "47                                       Modern Times    8.4  1936\n",
       "48                       Once Upon a Time in the West    8.4  1968\n",
       "49                              Nuovo Cinema Paradiso    8.4  1988\n",
       "50                                        Rear Window    8.4  1954"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=imdb_top50('https://www.imdb.com/chart/top/?ref_=nv_mv_250')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8457f6",
   "metadata": {},
   "source": [
    "## 3)Write a python program to display IMDB’s Top rated 50 Indian movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0645d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function that takes in a URL as input\n",
    "def top_indian_movies(url):\n",
    "   \n",
    "    # Send a GET request to the IMDb India Top Rated Movies page and parse its content using BeautifulSoup\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content,'html.parser')\n",
    "\n",
    "    # Find all td elements with a class attribute of 'titleColumn' and extract the movie name, rating, and year of release for the first 50 movies\n",
    "    top_movies = soup.find_all('td',class_='titleColumn')\n",
    "    movie_name=[]\n",
    "    movie_rating=[]\n",
    "    year_relese=[]\n",
    "\n",
    "    for movie in top_movies[:50]:\n",
    "        # Extract the movie name from the <a> element inside the <td> element and append it to the movie_name list\n",
    "        name=movie.a.text\n",
    "        movie_name.append(name)\n",
    "\n",
    "        # Extract the movie rating from the <strong> element inside the <td> element's parent <td> element with a class attribute of 'ratingColumn imdbRating' and append it to the movie_rating list\n",
    "        rating = movie.parent.find(\"td\", class_=\"ratingColumn imdbRating\").strong.text\n",
    "        movie_rating.append(rating)\n",
    "\n",
    "        # Extract the year of release from the <span> element inside the <td> element and append it to the year_relese list\n",
    "        year = movie.span.text.strip(\"()\")\n",
    "        year_relese.append(year)\n",
    "\n",
    "    # Create a Pandas DataFrame from the extracted movie data    \n",
    "    df = pd.DataFrame({\n",
    "            'Name':movie_name,\n",
    "            'Rating':movie_rating,\n",
    "            'Year':year_relese\n",
    "\n",
    "            })\n",
    "    # Set the DataFrame index to start from 1 instead of the default 0\n",
    "    df.index = range(1, len(df)+1)\n",
    "\n",
    "    # Return the DataFrame to display the top 50 Indian movies with their ratings and years of release\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b20c8dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ramayana: The Legend of Prince Rama</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rocketry: The Nambi Effect</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nayakan</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gol Maal</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>777 Charlie</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Anbe Sivam</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pariyerum Perumal</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Apur Sansar</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3 Idiots</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Manichitrathazhu</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Jai Bhim</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>#Home</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Soorarai Pottru</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Black Friday</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Kumbalangi Nights</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>C/o Kancharapalem</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Taare Zameen Par</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Kireedam</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Dangal</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Kaithi</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Jersey</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>96</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Maya Bazaar</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Natsamrat</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Drishyam 2</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Asuran</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Sita Ramam</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Thevar Magan</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Visaaranai</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Sarpatta Parambarai</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Thalapathi</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Nadodikkattu</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Pather Panchali</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Drishyam</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Thani Oruvan</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Jaane Bhi Do Yaaro</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Vada Chennai</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Aparajito</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Khosla Ka Ghosla!</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Sardar Udham</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Anniyan</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Ratsasan</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Chupke Chupke</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Gangs of Wasseypur</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Drishyam</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Mahanati</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Peranbu</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Bangalore Days</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Premam</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Satya</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Name Rating  Year\n",
       "1   Ramayana: The Legend of Prince Rama    8.6  1993\n",
       "2            Rocketry: The Nambi Effect    8.4  2022\n",
       "3                               Nayakan    8.4  1987\n",
       "4                              Gol Maal    8.4  1979\n",
       "5                           777 Charlie    8.4  2022\n",
       "6                            Anbe Sivam    8.4  2003\n",
       "7                     Pariyerum Perumal    8.4  2018\n",
       "8                           Apur Sansar    8.4  1959\n",
       "9                              3 Idiots    8.4  2009\n",
       "10                     Manichitrathazhu    8.3  1993\n",
       "11                             Jai Bhim    8.3  2021\n",
       "12                                #Home    8.3  2021\n",
       "13                      Soorarai Pottru    8.3  2020\n",
       "14                         Black Friday    8.3  2004\n",
       "15                    Kumbalangi Nights    8.3  2019\n",
       "16                    C/o Kancharapalem    8.3  2018\n",
       "17                     Taare Zameen Par    8.3  2007\n",
       "18                             Kireedam    8.3  1989\n",
       "19                               Dangal    8.3  2016\n",
       "20                               Kaithi    8.3  2019\n",
       "21                               Jersey    8.3  2019\n",
       "22                                   96    8.3  2018\n",
       "23                          Maya Bazaar    8.2  1957\n",
       "24                            Natsamrat    8.2  2016\n",
       "25                           Drishyam 2    8.2  2021\n",
       "26                               Asuran    8.2  2019\n",
       "27                           Sita Ramam    8.2  2022\n",
       "28                         Thevar Magan    8.2  1992\n",
       "29                           Visaaranai    8.2  2015\n",
       "30                  Sarpatta Parambarai    8.2  2021\n",
       "31                           Thalapathi    8.2  1991\n",
       "32                         Nadodikkattu    8.2  1987\n",
       "33                      Pather Panchali    8.2  1955\n",
       "34                             Drishyam    8.2  2013\n",
       "35                         Thani Oruvan    8.2  2015\n",
       "36                   Jaane Bhi Do Yaaro    8.2  1983\n",
       "37                         Vada Chennai    8.2  2018\n",
       "38                            Aparajito    8.2  1956\n",
       "39                    Khosla Ka Ghosla!    8.2  2006\n",
       "40                         Sardar Udham    8.2  2021\n",
       "41                              Anniyan    8.2  2005\n",
       "42                             Ratsasan    8.1  2018\n",
       "43                        Chupke Chupke    8.1  1975\n",
       "44                   Gangs of Wasseypur    8.1  2012\n",
       "45                             Drishyam    8.1  2015\n",
       "46                             Mahanati    8.1  2018\n",
       "47                              Peranbu    8.1  2018\n",
       "48                       Bangalore Days    8.1  2014\n",
       "49                               Premam    8.1  2015\n",
       "50                                Satya    8.1  1998"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=top_indian_movies(\"https://www.imdb.com/india/top-rated-indian-movies/\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a067d6",
   "metadata": {},
   "source": [
    "## 4) Write s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice) from https://presidentofindia.nic.in/former-presidents.htm and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9138789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function that takes in a URL as input\n",
    "def get_former_presidents(url):\n",
    "    # Send a GET request to the President of India website and parse its content using BeautifulSoup\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content,'html.parser')\n",
    "\n",
    "    # Create empty lists to store the names and terms of office of former presidents\n",
    "    name = []\n",
    "    term = []\n",
    "\n",
    "    # Extract the names of former presidents from the <h3> element inside the <div> element with a class attribute of 'presidentListing', and append them to the name list\n",
    "    for i in soup.find_all('div', class_=\"presidentListing\"):\n",
    "        name.append(i.h3.text.strip())\n",
    "\n",
    "    # Extract the terms of office of former presidents from the <p> element inside the <div> element with a class attribute of 'presidentListing', and append them to the term list    \n",
    "    for i in soup.find_all('div', class_=\"presidentListing\"):\n",
    "        term.append(i.p.text)\n",
    "\n",
    "    # Create a Pandas DataFrame from the extracted data   \n",
    "    df=pd.DataFrame({'NAME':name,'TERM OF OFFICE':term})\n",
    "\n",
    "    # Set the DataFrame index to start from 1 instead of the default 0\n",
    "    df.index = range(1, len(df)+1)\n",
    "\n",
    "    # Return the DataFrame to display the names and terms of office of former presidents\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9eeccbc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>TERM OF OFFICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shri Ram Nath Kovind (birth - 1945)</td>\n",
       "      <td>Term of Office: 25 July, 2017 to 25 July, 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shri Pranab Mukherjee (1935-2020)</td>\n",
       "      <td>Term of Office: 25 July, 2012 to 25 July, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Smt Pratibha Devisingh Patil (birth - 1934)</td>\n",
       "      <td>Term of Office: 25 July, 2007 to 25 July, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DR. A.P.J. Abdul Kalam (1931-2015)</td>\n",
       "      <td>Term of Office: 25 July, 2002 to 25 July, 2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Shri K. R. Narayanan (1920 - 2005)</td>\n",
       "      <td>Term of Office: 25 July, 1997 to 25 July, 2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dr Shankar Dayal Sharma (1918-1999)</td>\n",
       "      <td>Term of Office: 25 July, 1992 to 25 July, 1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Shri R Venkataraman (1910-2009)</td>\n",
       "      <td>Term of Office: 25 July, 1987 to 25 July, 1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Giani Zail Singh (1916-1994)</td>\n",
       "      <td>Term of Office: 25 July, 1982 to 25 July, 1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Shri Neelam Sanjiva Reddy (1913-1996)</td>\n",
       "      <td>Term of Office: 25 July, 1977 to 25 July, 1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Dr. Fakhruddin Ali Ahmed (1905-1977)</td>\n",
       "      <td>Term of Office: 24 August, 1974 to 11 February...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Shri Varahagiri Venkata Giri (1894-1980)</td>\n",
       "      <td>Term of Office: 3 May, 1969 to 20 July, 1969 a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Dr. Zakir Husain (1897-1969)</td>\n",
       "      <td>Term of Office: 13 May, 1967 to 3 May, 1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Dr. Sarvepalli Radhakrishnan (1888-1975)</td>\n",
       "      <td>Term of Office: 13 May, 1962 to 13 May, 1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Dr. Rajendra Prasad (1884-1963)</td>\n",
       "      <td>Term of Office: 26 January, 1950 to 13 May, 1962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           NAME  \\\n",
       "1           Shri Ram Nath Kovind (birth - 1945)   \n",
       "2             Shri Pranab Mukherjee (1935-2020)   \n",
       "3   Smt Pratibha Devisingh Patil (birth - 1934)   \n",
       "4            DR. A.P.J. Abdul Kalam (1931-2015)   \n",
       "5            Shri K. R. Narayanan (1920 - 2005)   \n",
       "6           Dr Shankar Dayal Sharma (1918-1999)   \n",
       "7               Shri R Venkataraman (1910-2009)   \n",
       "8                  Giani Zail Singh (1916-1994)   \n",
       "9         Shri Neelam Sanjiva Reddy (1913-1996)   \n",
       "10         Dr. Fakhruddin Ali Ahmed (1905-1977)   \n",
       "11     Shri Varahagiri Venkata Giri (1894-1980)   \n",
       "12                 Dr. Zakir Husain (1897-1969)   \n",
       "13     Dr. Sarvepalli Radhakrishnan (1888-1975)   \n",
       "14              Dr. Rajendra Prasad (1884-1963)   \n",
       "\n",
       "                                       TERM OF OFFICE  \n",
       "1     Term of Office: 25 July, 2017 to 25 July, 2022   \n",
       "2     Term of Office: 25 July, 2012 to 25 July, 2017   \n",
       "3     Term of Office: 25 July, 2007 to 25 July, 2012   \n",
       "4     Term of Office: 25 July, 2002 to 25 July, 2007   \n",
       "5     Term of Office: 25 July, 1997 to 25 July, 2002   \n",
       "6     Term of Office: 25 July, 1992 to 25 July, 1997   \n",
       "7     Term of Office: 25 July, 1987 to 25 July, 1992   \n",
       "8     Term of Office: 25 July, 1982 to 25 July, 1987   \n",
       "9     Term of Office: 25 July, 1977 to 25 July, 1982   \n",
       "10  Term of Office: 24 August, 1974 to 11 February...  \n",
       "11  Term of Office: 3 May, 1969 to 20 July, 1969 a...  \n",
       "12        Term of Office: 13 May, 1967 to 3 May, 1969  \n",
       "13       Term of Office: 13 May, 1962 to 13 May, 1967  \n",
       "14   Term of Office: 26 January, 1950 to 13 May, 1962  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_former_presidents(\"https://presidentofindia.nic.in/former-presidents.htm\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298d0fae",
   "metadata": {},
   "source": [
    "## 5) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\n",
    "### a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "### b) Top 10 ODI Batsmen along with the records of their team andrating.\n",
    "### c) Top 10 ODI bowlers along with the records of their team andrating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92522df6",
   "metadata": {},
   "source": [
    ".........................................................................................................................................................................................."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac8627a",
   "metadata": {},
   "source": [
    "#### a)Top 10 ODI teams in men’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43ee1b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function that takes in a URL as input\n",
    "def odi_teams(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the table containing the rankings data\n",
    "    table = soup.find(\"table\", class_=\"table\")\n",
    "\n",
    "    # Extract the data from the table and store it in lists\n",
    "    teams = []\n",
    "    matches = []\n",
    "    points = []\n",
    "    ratings = []\n",
    "\n",
    "    # Loop through each row in the table\n",
    "    for row in table.tbody.find_all(\"tr\"):\n",
    "        # Extract team name from the <span> element with class 'u-hide-phablet' and append to 'teams' list\n",
    "        team = row.find(\"span\", class_=\"u-hide-phablet\").text.strip()\n",
    "        teams.append(team)\n",
    "\n",
    "        # Extract number of matches from the third <td> element and append to 'matches' list\n",
    "        match = row.find_all(\"td\")[2].text.strip()\n",
    "        matches.append(match)\n",
    "\n",
    "        # Extract points from the fourth <td> element and append to 'points' list\n",
    "        point = row.find_all(\"td\")[3].text.strip()\n",
    "        points.append(point)\n",
    "\n",
    "        # Extract rating from the fifth <td> element and append to 'ratings' list\n",
    "        rating = row.find_all(\"td\")[4].text.strip()\n",
    "        ratings.append(rating)\n",
    "\n",
    "    # Create a data frame from the lists\n",
    "    data = {\"Team\": teams[:10], \"Matches\": matches[:10], \"Points\": points[:10], \"Rating\": ratings[:10]}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Set the DataFrame index to start from 1 instead of the default 0\n",
    "    df.index = range(1, len(df)+1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "041193ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team</th>\n",
       "      <th>Matches</th>\n",
       "      <th>Points</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Australia</td>\n",
       "      <td>35</td>\n",
       "      <td>3,965</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>31</td>\n",
       "      <td>3,504</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India</td>\n",
       "      <td>47</td>\n",
       "      <td>5,294</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>England</td>\n",
       "      <td>36</td>\n",
       "      <td>3,988</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>25</td>\n",
       "      <td>2,649</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>31</td>\n",
       "      <td>3,141</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>38</td>\n",
       "      <td>3,625</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>36</td>\n",
       "      <td>3,099</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>West Indies</td>\n",
       "      <td>43</td>\n",
       "      <td>3,105</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>20</td>\n",
       "      <td>1,419</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Team Matches Points Rating\n",
       "1      Australia      35  3,965    113\n",
       "2    New Zealand      31  3,504    113\n",
       "3          India      47  5,294    113\n",
       "4        England      36  3,988    111\n",
       "5       Pakistan      25  2,649    106\n",
       "6   South Africa      31  3,141    101\n",
       "7     Bangladesh      38  3,625     95\n",
       "8      Sri Lanka      36  3,099     86\n",
       "9    West Indies      43  3,105     72\n",
       "10   Afghanistan      20  1,419     71"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = odi_teams(\"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461bf18f",
   "metadata": {},
   "source": [
    "#### b) Top 10 ODI Batsmen along with the records of their team andrating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae9a4fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function that takes in a URL as input\n",
    "def top_batsmenss(url):\n",
    "    # send request to the server \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # create soup object\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # fetching  the top player data\n",
    "    name = []\n",
    "    team = []\n",
    "    rate =[]\n",
    "    a=soup.find('div', 'rankings-block__banner--name').text\n",
    "    b=soup.find('div','rankings-block__banner--nationality').text[2:5]\n",
    "    c=soup.find('div','rankings-block__banner--rating').text\n",
    "    name.append(a)\n",
    "    team.append(b)\n",
    "    rate.append(c)\n",
    "    # extract the names of the batsmens\n",
    "    namesss = []\n",
    "    for i in soup.find_all('td', 'table-body__cell name'):\n",
    "        namesss.append(i.text.strip())\n",
    "    # extract the teams of  the players\n",
    "    teams = []\n",
    "    for i in soup.find_all('td', 'table-body__cell nationality-logo'):\n",
    "        teams.append(i.text.strip())\n",
    "    # extract the ratings of the batsmens\n",
    "    rating = []\n",
    "    for i in soup.find_all('td', 'table-body__cell u-text-right rating'):\n",
    "        rating.append(i.text.strip())\n",
    "    # adding the data together \n",
    "    names = name + namesss\n",
    "    teams = team + teams\n",
    "    rating = rate + rating\n",
    "    \n",
    "    # create dataframe\n",
    "    df = pd.DataFrame({\n",
    "        'Batsmen':names,\n",
    "        'Team':teams,\n",
    "        'Rating':rating\n",
    "    })\n",
    "    # Set the DataFrame index to start from 1 instead of the default 0\n",
    "    df.index = range(1, len(df)+1)\n",
    "    # return the DataFrame\n",
    "    return df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e53b8639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batsmen</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Babar Azam</td>\n",
       "      <td>PAK</td>\n",
       "      <td>887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rassie van der Dussen</td>\n",
       "      <td>SA</td>\n",
       "      <td>777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Imam-ul-Haq</td>\n",
       "      <td>PAK</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shubman Gill</td>\n",
       "      <td>IND</td>\n",
       "      <td>738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>David Warner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Virat Kohli</td>\n",
       "      <td>IND</td>\n",
       "      <td>719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Quinton de Kock</td>\n",
       "      <td>SA</td>\n",
       "      <td>718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Rohit Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Steve Smith</td>\n",
       "      <td>AUS</td>\n",
       "      <td>702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Fakhar Zaman</td>\n",
       "      <td>PAK</td>\n",
       "      <td>699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Batsmen Team Rating\n",
       "1              Babar Azam  PAK    887\n",
       "2   Rassie van der Dussen   SA    777\n",
       "3             Imam-ul-Haq  PAK    740\n",
       "4            Shubman Gill  IND    738\n",
       "5            David Warner  AUS    726\n",
       "6             Virat Kohli  IND    719\n",
       "7         Quinton de Kock   SA    718\n",
       "8            Rohit Sharma  IND    707\n",
       "9             Steve Smith  AUS    702\n",
       "10           Fakhar Zaman  PAK    699"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = top_batsmenss('https://www.icc-cricket.com/rankings/mens/player-rankings/odi')\n",
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16bac66",
   "metadata": {},
   "source": [
    "#### c) Top 10 ODI bowlers along with the records of their team andrating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b13cb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function that takes in a URL as input\n",
    "def get_top_10_ODI_bowlers(url):\n",
    "    # Send a request to the ICC ODI rankings web page and get the response\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content using Beautiful Soup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the table containing the top 10 ODI bowlers\n",
    "    table = soup.find('table', attrs={'class': 'table'})\n",
    "\n",
    "    # Extract the data from the table and store it in a list of dictionaries\n",
    "    data = []\n",
    "    rows = table.find_all('tr')\n",
    "    for row in rows[1:11]: # We only need the top 10 bowlers\n",
    "        cells = row.find_all('td')\n",
    "        player_name = cells[1].text.strip()\n",
    "        team = cells[2].text.strip()\n",
    "        rating = cells[3].text.strip()\n",
    "        data.append({\n",
    "            'Player Name': player_name,\n",
    "            'Team': team,\n",
    "            'Rating': rating\n",
    "        })\n",
    "\n",
    "    # Create a Pandas DataFrame from the list of dictionaries and return it\n",
    "    df = pd.DataFrame(data)\n",
    "    # Set the DataFrame index to start from 1 instead of the default 0\n",
    "    df.index = range(1, len(df)+1)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7429fa52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player Name</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Josh Hazlewood</td>\n",
       "      <td>AUS</td>\n",
       "      <td>705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trent Boult</td>\n",
       "      <td>NZ</td>\n",
       "      <td>694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mohammed Siraj</td>\n",
       "      <td>IND</td>\n",
       "      <td>691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mitchell Starc</td>\n",
       "      <td>AUS</td>\n",
       "      <td>686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Matt Henry</td>\n",
       "      <td>NZ</td>\n",
       "      <td>676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Rashid Khan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Adam Zampa</td>\n",
       "      <td>AUS</td>\n",
       "      <td>652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shaheen Afridi</td>\n",
       "      <td>PAK</td>\n",
       "      <td>641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mujeeb Ur Rahman</td>\n",
       "      <td>AFG</td>\n",
       "      <td>637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Shakib Al Hasan</td>\n",
       "      <td>BAN</td>\n",
       "      <td>636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Player Name Team Rating\n",
       "1     Josh Hazlewood  AUS    705\n",
       "2        Trent Boult   NZ    694\n",
       "3     Mohammed Siraj  IND    691\n",
       "4     Mitchell Starc  AUS    686\n",
       "5         Matt Henry   NZ    676\n",
       "6        Rashid Khan  AFG    659\n",
       "7         Adam Zampa  AUS    652\n",
       "8     Shaheen Afridi  PAK    641\n",
       "9   Mujeeb Ur Rahman  AFG    637\n",
       "10   Shakib Al Hasan  BAN    636"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top = get_top_10_ODI_bowlers('https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling')\n",
    "top"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b61add",
   "metadata": {},
   "source": [
    "## 6) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\n",
    "#### a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "#### b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "#### c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b079684",
   "metadata": {},
   "source": [
    ".........................................................................................................................................."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0bf802",
   "metadata": {},
   "source": [
    "#### a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f32df0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function that takes in a URL as input\n",
    "def icc_womens_team(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the table containing the rankings data\n",
    "    table = soup.find(\"table\", class_=\"table\")\n",
    "\n",
    "    # Extract the data from the table and store it in lists\n",
    "    teams = []\n",
    "    matches = []\n",
    "    points = []\n",
    "    ratings = []\n",
    "\n",
    "    for row in table.tbody.find_all(\"tr\"):\n",
    "        team = row.find(\"span\", class_=\"u-hide-phablet\").text.strip()\n",
    "        teams.append(team)\n",
    "\n",
    "        match = row.find_all(\"td\")[2].text.strip()\n",
    "        matches.append(match)\n",
    "\n",
    "        point = row.find_all(\"td\")[3].text.strip()\n",
    "        points.append(point)\n",
    "\n",
    "        rating = row.find_all(\"td\")[4].text.strip()\n",
    "        ratings.append(rating)\n",
    "\n",
    "    # Create a data frame from the lists\n",
    "    data = {\"Team\": teams[:10], \"Matches\": matches[:10], \"Points\": points[:10], \"Rating\": ratings[:10]}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Set the DataFrame index to start from 1 instead of the default 0\n",
    "    df.index = range(1, len(df)+1)\n",
    "\n",
    "    # Return the data frame\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08168b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team</th>\n",
       "      <th>Matches</th>\n",
       "      <th>Points</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Australia</td>\n",
       "      <td>21</td>\n",
       "      <td>3,603</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>England</td>\n",
       "      <td>28</td>\n",
       "      <td>3,342</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>26</td>\n",
       "      <td>3,098</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>India</td>\n",
       "      <td>27</td>\n",
       "      <td>2,820</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>25</td>\n",
       "      <td>2,553</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>West Indies</td>\n",
       "      <td>27</td>\n",
       "      <td>2,535</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>13</td>\n",
       "      <td>983</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>11</td>\n",
       "      <td>821</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>27</td>\n",
       "      <td>1,678</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>8</td>\n",
       "      <td>353</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Team Matches Points Rating\n",
       "1      Australia      21  3,603    172\n",
       "2        England      28  3,342    119\n",
       "3   South Africa      26  3,098    119\n",
       "4          India      27  2,820    104\n",
       "5    New Zealand      25  2,553    102\n",
       "6    West Indies      27  2,535     94\n",
       "7     Bangladesh      13    983     76\n",
       "8       Thailand      11    821     75\n",
       "9       Pakistan      27  1,678     62\n",
       "10     Sri Lanka       8    353     44"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_odi_wo = icc_womens_team('https://www.icc-cricket.com/rankings/womens/team-rankings/odi')\n",
    "top_odi_wo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a3e88f",
   "metadata": {},
   "source": [
    "#### b) Top 10 women’s ODI Batting players along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e81df612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function that takes in a URL as input\n",
    "def top_10_women_batting_players(url):\n",
    "   \n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the table containing the rankings data\n",
    "    table = soup.find('table', {'class': 'table rankings-table'})\n",
    "\n",
    "    # Create an empty list to store the data\n",
    "    data = []\n",
    "\n",
    "    # Find all the rows in the table\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    # Loop through the rows and extract the data\n",
    "    for row in rows[1:11]: # Take only first 10 rows for top 10 players\n",
    "        cols = row.find_all('td')\n",
    "        cols = [col.text.strip() for col in cols]\n",
    "        player = cols[1]\n",
    "        team = cols[2]\n",
    "        rating = cols[3]\n",
    "        data.append([player, team, rating])\n",
    "\n",
    "    # Create a Pandas data frame with the data\n",
    "    df = pd.DataFrame(data, columns=['Player', 'Team', 'Rating'])\n",
    "\n",
    "    # Add position numbers to the data frame\n",
    "    df.index += 1\n",
    "\n",
    "    # Return the data frame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a767782d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alyssa Healy</td>\n",
       "      <td>AUS</td>\n",
       "      <td>762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beth Mooney</td>\n",
       "      <td>AUS</td>\n",
       "      <td>754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Laura Wolvaardt</td>\n",
       "      <td>SA</td>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Meg Lanning</td>\n",
       "      <td>AUS</td>\n",
       "      <td>717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Harmanpreet Kaur</td>\n",
       "      <td>IND</td>\n",
       "      <td>716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Smriti Mandhana</td>\n",
       "      <td>IND</td>\n",
       "      <td>714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Chamari Athapaththu</td>\n",
       "      <td>SL</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Amy Satterthwaite</td>\n",
       "      <td>NZ</td>\n",
       "      <td>641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Ellyse Perry</td>\n",
       "      <td>AUS</td>\n",
       "      <td>626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Player Team Rating\n",
       "1          Alyssa Healy  AUS    762\n",
       "2           Beth Mooney  AUS    754\n",
       "3       Laura Wolvaardt   SA    732\n",
       "4        Natalie Sciver  ENG    731\n",
       "5           Meg Lanning  AUS    717\n",
       "6      Harmanpreet Kaur  IND    716\n",
       "7       Smriti Mandhana  IND    714\n",
       "8   Chamari Athapaththu   SL    655\n",
       "9     Amy Satterthwaite   NZ    641\n",
       "10         Ellyse Perry  AUS    626"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wo_batsmen = top_10_women_batting_players('https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting')\n",
    "wo_batsmen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0da6779",
   "metadata": {},
   "source": [
    "#### c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fef7e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function that takes in a URL as input\n",
    "def top_10_women_allrounders(url):\n",
    "    # URL of the ICC women's ODI all-rounder rankings\n",
    "   \n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the table containing the rankings data\n",
    "    table = soup.find('table', {'class': 'table rankings-table'})\n",
    "\n",
    "    # Create an empty list to store the data\n",
    "    data = []\n",
    "\n",
    "    # Find all the rows in the table\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    # Loop through the rows and extract the data\n",
    "    for row in rows[1:11]: # Take only first 10 rows for top 10 players\n",
    "        cols = row.find_all('td')\n",
    "        cols = [col.text.strip() for col in cols]\n",
    "        player = cols[1]\n",
    "        team = cols[2]\n",
    "        rating = cols[3]\n",
    "        data.append([player, team, rating])\n",
    "\n",
    "    # Create a Pandas data frame with the data\n",
    "    df = pd.DataFrame(data, columns=['Player', 'Team', 'Rating'])\n",
    "\n",
    "    # Add position numbers to the data frame\n",
    "    df.index += 1\n",
    "\n",
    "    # Return the data frame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15f1809d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hayley Matthews</td>\n",
       "      <td>WI</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ellyse Perry</td>\n",
       "      <td>AUS</td>\n",
       "      <td>366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Marizanne Kapp</td>\n",
       "      <td>SA</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Amelia Kerr</td>\n",
       "      <td>NZ</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Deepti Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ashleigh Gardner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Jess Jonassen</td>\n",
       "      <td>AUS</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Nida Dar</td>\n",
       "      <td>PAK</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sophie Ecclestone</td>\n",
       "      <td>ENG</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Player Team Rating\n",
       "1     Hayley Matthews   WI    373\n",
       "2      Natalie Sciver  ENG    371\n",
       "3        Ellyse Perry  AUS    366\n",
       "4      Marizanne Kapp   SA    349\n",
       "5         Amelia Kerr   NZ    336\n",
       "6       Deepti Sharma  IND    322\n",
       "7    Ashleigh Gardner  AUS    292\n",
       "8       Jess Jonassen  AUS    250\n",
       "9            Nida Dar  PAK    232\n",
       "10  Sophie Ecclestone  ENG    205"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wo_all = top_10_women_allrounders('https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder')\n",
    "wo_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd5286b",
   "metadata": {},
   "source": [
    "## 7) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and make data frame-\n",
    "- i) Headline\n",
    "- ii) Time\n",
    "- iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4afc1c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function that takes in a URL as input\n",
    "def scrape_cnbc_world_news(url):\n",
    "    #Send a request to the CNBC world news page and get the response\n",
    "    page = requests.get(url)\n",
    "    #Parse the HTML content using Beautiful Soup\n",
    "    soup = BeautifulSoup(page.content,'html.parser')\n",
    "\n",
    "    #Initialize empty lists to store the news headlines, time and links\n",
    "    Headline = []\n",
    "    Time = []\n",
    "    News_Link=[]\n",
    "\n",
    "\n",
    "    #Extract the news headlines, time and links from the parsed HTML content and append to the respective lists\n",
    "\n",
    "    for i in soup.find_all('a',class_=\"LatestNews-headline\"):\n",
    "        Headline.append(i.text)\n",
    "\n",
    "    for i in soup.find_all('a',class_=\"LatestNews-headline\"):\n",
    "        News_Link.append(i.get('href'))\n",
    "\n",
    "    for i in soup.find_all('time',class_=\"LatestNews-timestamp\"):\n",
    "        Time.append(i.text)\n",
    "\n",
    "\n",
    "    #Create a Pandas DataFrame from the lists    \n",
    "    df=pd.DataFrame({'Headline':Headline,'Time':Time,'News_Link':News_Link})\n",
    "\n",
    "    #Reset the index of the DataFrame and return it\n",
    "    df.index = range(1, len(df)+1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c53793d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Time</th>\n",
       "      <th>News_Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Electric car sales in 2022 hit over 10 million...</td>\n",
       "      <td>35 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/04/26/electric-car-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CNBC Daily Open: Big Tech surpasses expectations</td>\n",
       "      <td>42 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/04/26/stock-markets-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>With Russia's role in the global energy system...</td>\n",
       "      <td>50 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/04/26/russias-role-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SK Hynix reports record quarterly operating lo...</td>\n",
       "      <td>56 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/04/26/sk-hynix-repor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>European markets head for negative open as ban...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/04/26/european-marke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Here's how to spend King Charles III's coronat...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/04/26/how-to-spend-k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Here are the top 25 companies to work for in I...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/04/26/here-are-the-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Alphabet's quarter delivers but doesn't dazzle...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/04/25/alphabets-quar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>'The hard money is now,' Stifel's chief strate...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/04/25/easy-money-is-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>China's EV players are starting to compete on ...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/04/26/chinas-ev-play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Want to buy tech stocks? Here's how to manage ...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/04/26/fund-manager-o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Missed the A.I. rally? HSBC names 4 Chinese st...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/04/26/missed-the-ai-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Asia markets mixed as Wall Street banking fear...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/04/26/asia-markets-w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Microsoft leans on its AI prowess to deliver a...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/04/25/microsoft-lean...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CNBC Daily Open: Big Tech beats expectations</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/04/26/stock-markets-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Cramer: First Republic’s 'miserable moment' is...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/04/25/cramer-first-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>The No. 1 mistake this self-made millionaire s...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/04/25/tori-dunlap-no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Harvard-trained expert: Use this 3-word respon...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/04/25/etiquette-expe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Stock futures rise Tuesday night after Microso...</td>\n",
       "      <td>9 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/04/25/stock-market-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Stocks making the biggest moves after hours: E...</td>\n",
       "      <td>9 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/04/25/stocks-moving-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Illumina unveils plans to cut costs as it face...</td>\n",
       "      <td>10 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/04/25/illumina-plans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>General Motors earnings beat expectations. Her...</td>\n",
       "      <td>10 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/04/25/general-motors...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Here's an update on the 10 core holdings in ou...</td>\n",
       "      <td>10 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/04/25/heres-an-updat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>GOP debt ceiling bill faces first test as anal...</td>\n",
       "      <td>11 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/04/25/debt-ceiling-g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Alphabet authorizes $70 billion buyback</td>\n",
       "      <td>11 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/04/25/google-authori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Google's cloud business turns profitable for t...</td>\n",
       "      <td>11 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/04/25/googles-cloud-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Warner Bros. Discovery previews a stacked film...</td>\n",
       "      <td>11 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/04/25/cinemacon-2023...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Chipotle Mexican Grill's restaurant traffic gr...</td>\n",
       "      <td>11 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/04/25/chipotle-mexic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>These mid-cap funds can give investors growth ...</td>\n",
       "      <td>12 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/04/25/these-mid-cap-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>The banking crisis is having a slow-burn impac...</td>\n",
       "      <td>12 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/04/25/the-banking-cr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Headline          Time  \\\n",
       "1   Electric car sales in 2022 hit over 10 million...    35 Min Ago   \n",
       "2    CNBC Daily Open: Big Tech surpasses expectations    42 Min Ago   \n",
       "3   With Russia's role in the global energy system...    50 Min Ago   \n",
       "4   SK Hynix reports record quarterly operating lo...    56 Min Ago   \n",
       "5   European markets head for negative open as ban...   2 Hours Ago   \n",
       "6   Here's how to spend King Charles III's coronat...   4 Hours Ago   \n",
       "7   Here are the top 25 companies to work for in I...   5 Hours Ago   \n",
       "8   Alphabet's quarter delivers but doesn't dazzle...   6 Hours Ago   \n",
       "9   'The hard money is now,' Stifel's chief strate...   6 Hours Ago   \n",
       "10  China's EV players are starting to compete on ...   7 Hours Ago   \n",
       "11  Want to buy tech stocks? Here's how to manage ...   7 Hours Ago   \n",
       "12  Missed the A.I. rally? HSBC names 4 Chinese st...   7 Hours Ago   \n",
       "13  Asia markets mixed as Wall Street banking fear...   7 Hours Ago   \n",
       "14  Microsoft leans on its AI prowess to deliver a...   7 Hours Ago   \n",
       "15       CNBC Daily Open: Big Tech beats expectations   7 Hours Ago   \n",
       "16  Cramer: First Republic’s 'miserable moment' is...   7 Hours Ago   \n",
       "17  The No. 1 mistake this self-made millionaire s...   8 Hours Ago   \n",
       "18  Harvard-trained expert: Use this 3-word respon...   8 Hours Ago   \n",
       "19  Stock futures rise Tuesday night after Microso...   9 Hours Ago   \n",
       "20  Stocks making the biggest moves after hours: E...   9 Hours Ago   \n",
       "21  Illumina unveils plans to cut costs as it face...  10 Hours Ago   \n",
       "22  General Motors earnings beat expectations. Her...  10 Hours Ago   \n",
       "23  Here's an update on the 10 core holdings in ou...  10 Hours Ago   \n",
       "24  GOP debt ceiling bill faces first test as anal...  11 Hours Ago   \n",
       "25            Alphabet authorizes $70 billion buyback  11 Hours Ago   \n",
       "26  Google's cloud business turns profitable for t...  11 Hours Ago   \n",
       "27  Warner Bros. Discovery previews a stacked film...  11 Hours Ago   \n",
       "28  Chipotle Mexican Grill's restaurant traffic gr...  11 Hours Ago   \n",
       "29  These mid-cap funds can give investors growth ...  12 Hours Ago   \n",
       "30  The banking crisis is having a slow-burn impac...  12 Hours Ago   \n",
       "\n",
       "                                            News_Link  \n",
       "1   https://www.cnbc.com/2023/04/26/electric-car-s...  \n",
       "2   https://www.cnbc.com/2023/04/26/stock-markets-...  \n",
       "3   https://www.cnbc.com/2023/04/26/russias-role-i...  \n",
       "4   https://www.cnbc.com/2023/04/26/sk-hynix-repor...  \n",
       "5   https://www.cnbc.com/2023/04/26/european-marke...  \n",
       "6   https://www.cnbc.com/2023/04/26/how-to-spend-k...  \n",
       "7   https://www.cnbc.com/2023/04/26/here-are-the-t...  \n",
       "8   https://www.cnbc.com/2023/04/25/alphabets-quar...  \n",
       "9   https://www.cnbc.com/2023/04/25/easy-money-is-...  \n",
       "10  https://www.cnbc.com/2023/04/26/chinas-ev-play...  \n",
       "11  https://www.cnbc.com/2023/04/26/fund-manager-o...  \n",
       "12  https://www.cnbc.com/2023/04/26/missed-the-ai-...  \n",
       "13  https://www.cnbc.com/2023/04/26/asia-markets-w...  \n",
       "14  https://www.cnbc.com/2023/04/25/microsoft-lean...  \n",
       "15  https://www.cnbc.com/2023/04/26/stock-markets-...  \n",
       "16  https://www.cnbc.com/2023/04/25/cramer-first-r...  \n",
       "17  https://www.cnbc.com/2023/04/25/tori-dunlap-no...  \n",
       "18  https://www.cnbc.com/2023/04/25/etiquette-expe...  \n",
       "19  https://www.cnbc.com/2023/04/25/stock-market-t...  \n",
       "20  https://www.cnbc.com/2023/04/25/stocks-moving-...  \n",
       "21  https://www.cnbc.com/2023/04/25/illumina-plans...  \n",
       "22  https://www.cnbc.com/2023/04/25/general-motors...  \n",
       "23  https://www.cnbc.com/2023/04/25/heres-an-updat...  \n",
       "24  https://www.cnbc.com/2023/04/25/debt-ceiling-g...  \n",
       "25  https://www.cnbc.com/2023/04/25/google-authori...  \n",
       "26  https://www.cnbc.com/2023/04/25/googles-cloud-...  \n",
       "27  https://www.cnbc.com/2023/04/25/cinemacon-2023...  \n",
       "28  https://www.cnbc.com/2023/04/25/chipotle-mexic...  \n",
       "29  https://www.cnbc.com/2023/04/25/these-mid-cap-...  \n",
       "30  https://www.cnbc.com/2023/04/25/the-banking-cr...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = scrape_cnbc_world_news('https://www.cnbc.com/world/?region=world')\n",
    "news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37456ba",
   "metadata": {},
   "source": [
    "## 8) Write a python program to scrape the details of most downloaded articles from AI in last 90 days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles Scrape below mentioned details and make data frame\n",
    "- i) Paper Title\n",
    "- ii) Authors\n",
    "- iii) Published Date\n",
    "- iv) Paper URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80a5cec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function that takes in a URL as input\n",
    "def most_downloaded_ai_articles(url):\n",
    "    # Send a GET request to the URL and parse the HTML content using BeautifulSoup\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    # Create empty lists to store the data\n",
    "    Title = []\n",
    "    Authors = []\n",
    "    Publish = []\n",
    "    URL = []\n",
    "\n",
    "    # Find the relevant HTML elements and extract the data\n",
    "    for i in soup.find_all('h2', class_=\"sc-1qrq3sd-1 gRGSUS sc-1nmom32-0 sc-1nmom32-1 btcbYu goSKRg\"):\n",
    "        Title.append(i.text)\n",
    "\n",
    "    for i in soup.find_all('span', class_=\"sc-1w3fpd7-0 dnCnAO\"):\n",
    "        Authors.append(i.text)\n",
    "\n",
    "    for i in soup.find_all('span', class_='sc-1thf9ly-2 dvggWt'):\n",
    "        Publish.append(i.text)\n",
    "\n",
    "    for i in soup.find_all('a', class_=\"sc-5smygv-0 fIXTHm\"):\n",
    "        URL.append(i.get('href'))\n",
    "\n",
    "    # Create a Pandas DataFrame from the lists\n",
    "    df = pd.DataFrame({' Paper Title': Title, 'Authors': Authors, 'Published Date': Publish, 'Paper URL': URL})\n",
    "\n",
    "    # Reset the index of the DataFrame and return it\n",
    "    df.index = range(1, len(df) + 1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3734b83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Published Date</th>\n",
       "      <th>Paper URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Reward is enough</td>\n",
       "      <td>Silver, David, Singh, Satinder, Precup, Doina,...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Making sense of raw input</td>\n",
       "      <td>Evans, Richard, Bošnjak, Matko and 5 more</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Law and logic: A review from an argumentation ...</td>\n",
       "      <td>Prakken, Henry, Sartor, Giovanni</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Creativity and artificial intelligence</td>\n",
       "      <td>Boden, Margaret A.</td>\n",
       "      <td>August 1998</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Artificial cognition for social human–robot in...</td>\n",
       "      <td>Lemaignan, Séverin, Warnier, Mathieu and 3 more</td>\n",
       "      <td>June 2017</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Explanation in artificial intelligence: Insigh...</td>\n",
       "      <td>Miller, Tim</td>\n",
       "      <td>February 2019</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Making sense of sensory input</td>\n",
       "      <td>Evans, Richard, Hernández-Orallo, José and 3 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Conflict-based search for optimal multi-agent ...</td>\n",
       "      <td>Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...</td>\n",
       "      <td>February 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Between MDPs and semi-MDPs: A framework for te...</td>\n",
       "      <td>Sutton, Richard S., Precup, Doina, Singh, Sati...</td>\n",
       "      <td>August 1999</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The Hanabi challenge: A new frontier for AI re...</td>\n",
       "      <td>Bard, Nolan, Foerster, Jakob N. and 13 more</td>\n",
       "      <td>March 2020</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Evaluating XAI: A comparison of rule-based and...</td>\n",
       "      <td>van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...</td>\n",
       "      <td>February 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Argumentation in artificial intelligence</td>\n",
       "      <td>Bench-Capon, T.J.M., Dunne, Paul E.</td>\n",
       "      <td>October 2007</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Algorithms for computing strategies in two-pla...</td>\n",
       "      <td>Bošanský, Branislav, Lisý, Viliam and 3 more</td>\n",
       "      <td>August 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Multiple object tracking: A literature review</td>\n",
       "      <td>Luo, Wenhan, Xing, Junliang and 4 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Selection of relevant features and examples in...</td>\n",
       "      <td>Blum, Avrim L., Langley, Pat</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>A survey of inverse reinforcement learning: Ch...</td>\n",
       "      <td>Arora, Saurabh, Doshi, Prashant</td>\n",
       "      <td>August 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Explaining individual predictions when feature...</td>\n",
       "      <td>Aas, Kjersti, Jullum, Martin, Løland, Anders</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>A review of possible effects of cognitive bias...</td>\n",
       "      <td>Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...</td>\n",
       "      <td>June 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Integrating social power into the decision-mak...</td>\n",
       "      <td>Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.</td>\n",
       "      <td>December 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>“That's (not) the output I expected!” On the r...</td>\n",
       "      <td>Riveiro, Maria, Thill, Serge</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Explaining black-box classifiers using post-ho...</td>\n",
       "      <td>Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...</td>\n",
       "      <td>May 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Algorithm runtime prediction: Methods &amp; evalua...</td>\n",
       "      <td>Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...</td>\n",
       "      <td>January 2014</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Wrappers for feature subset selection</td>\n",
       "      <td>Kohavi, Ron, John, George H.</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Commonsense visual sensemaking for autonomous ...</td>\n",
       "      <td>Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Quantum computation, quantum theory and AI</td>\n",
       "      <td>Ying, Mingsheng</td>\n",
       "      <td>February 2010</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Paper Title  \\\n",
       "1                                    Reward is enough   \n",
       "2                           Making sense of raw input   \n",
       "3   Law and logic: A review from an argumentation ...   \n",
       "4              Creativity and artificial intelligence   \n",
       "5   Artificial cognition for social human–robot in...   \n",
       "6   Explanation in artificial intelligence: Insigh...   \n",
       "7                       Making sense of sensory input   \n",
       "8   Conflict-based search for optimal multi-agent ...   \n",
       "9   Between MDPs and semi-MDPs: A framework for te...   \n",
       "10  The Hanabi challenge: A new frontier for AI re...   \n",
       "11  Evaluating XAI: A comparison of rule-based and...   \n",
       "12           Argumentation in artificial intelligence   \n",
       "13  Algorithms for computing strategies in two-pla...   \n",
       "14      Multiple object tracking: A literature review   \n",
       "15  Selection of relevant features and examples in...   \n",
       "16  A survey of inverse reinforcement learning: Ch...   \n",
       "17  Explaining individual predictions when feature...   \n",
       "18  A review of possible effects of cognitive bias...   \n",
       "19  Integrating social power into the decision-mak...   \n",
       "20  “That's (not) the output I expected!” On the r...   \n",
       "21  Explaining black-box classifiers using post-ho...   \n",
       "22  Algorithm runtime prediction: Methods & evalua...   \n",
       "23              Wrappers for feature subset selection   \n",
       "24  Commonsense visual sensemaking for autonomous ...   \n",
       "25         Quantum computation, quantum theory and AI   \n",
       "\n",
       "                                              Authors  Published Date  \\\n",
       "1   Silver, David, Singh, Satinder, Precup, Doina,...    October 2021   \n",
       "2           Evans, Richard, Bošnjak, Matko and 5 more    October 2021   \n",
       "3                   Prakken, Henry, Sartor, Giovanni     October 2015   \n",
       "4                                 Boden, Margaret A.      August 1998   \n",
       "5     Lemaignan, Séverin, Warnier, Mathieu and 3 more       June 2017   \n",
       "6                                        Miller, Tim    February 2019   \n",
       "7   Evans, Richard, Hernández-Orallo, José and 3 more      April 2021   \n",
       "8   Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...   February 2015   \n",
       "9   Sutton, Richard S., Precup, Doina, Singh, Sati...     August 1999   \n",
       "10        Bard, Nolan, Foerster, Jakob N. and 13 more      March 2020   \n",
       "11  van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...   February 2021   \n",
       "12               Bench-Capon, T.J.M., Dunne, Paul E.     October 2007   \n",
       "13       Bošanský, Branislav, Lisý, Viliam and 3 more     August 2016   \n",
       "14             Luo, Wenhan, Xing, Junliang and 4 more      April 2021   \n",
       "15                      Blum, Avrim L., Langley, Pat    December 1997   \n",
       "16                   Arora, Saurabh, Doshi, Prashant      August 2021   \n",
       "17      Aas, Kjersti, Jullum, Martin, Løland, Anders   September 2021   \n",
       "18  Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...       June 2021   \n",
       "19    Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.    December 2016   \n",
       "20                      Riveiro, Maria, Thill, Serge   September 2021   \n",
       "21  Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...        May 2021   \n",
       "22  Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...    January 2014   \n",
       "23                      Kohavi, Ron, John, George H.    December 1997   \n",
       "24  Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...    October 2021   \n",
       "25                                   Ying, Mingsheng    February 2010   \n",
       "\n",
       "                                            Paper URL  \n",
       "1   https://www.sciencedirect.com/science/article/...  \n",
       "2   https://www.sciencedirect.com/science/article/...  \n",
       "3   https://www.sciencedirect.com/science/article/...  \n",
       "4   https://www.sciencedirect.com/science/article/...  \n",
       "5   https://www.sciencedirect.com/science/article/...  \n",
       "6   https://www.sciencedirect.com/science/article/...  \n",
       "7   https://www.sciencedirect.com/science/article/...  \n",
       "8   https://www.sciencedirect.com/science/article/...  \n",
       "9   https://www.sciencedirect.com/science/article/...  \n",
       "10  https://www.sciencedirect.com/science/article/...  \n",
       "11  https://www.sciencedirect.com/science/article/...  \n",
       "12  https://www.sciencedirect.com/science/article/...  \n",
       "13  https://www.sciencedirect.com/science/article/...  \n",
       "14  https://www.sciencedirect.com/science/article/...  \n",
       "15  https://www.sciencedirect.com/science/article/...  \n",
       "16  https://www.sciencedirect.com/science/article/...  \n",
       "17  https://www.sciencedirect.com/science/article/...  \n",
       "18  https://www.sciencedirect.com/science/article/...  \n",
       "19  https://www.sciencedirect.com/science/article/...  \n",
       "20  https://www.sciencedirect.com/science/article/...  \n",
       "21  https://www.sciencedirect.com/science/article/...  \n",
       "22  https://www.sciencedirect.com/science/article/...  \n",
       "23  https://www.sciencedirect.com/science/article/...  \n",
       "24  https://www.sciencedirect.com/science/article/...  \n",
       "25  https://www.sciencedirect.com/science/article/...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = most_downloaded_ai_articles('https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles')\n",
    "articles "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac50aeb",
   "metadata": {},
   "source": [
    "## 9) Write a python program to scrape mentioned details from dineout.co.in and make data frame-\n",
    "- i) Restaurant name\n",
    "- ii) Cuisine\n",
    "- iii) Location\n",
    "- iv) Ratings\n",
    "- v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bdc5dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function that takes in a URL as input\n",
    "def dineout_restaurants(url):\n",
    "   \n",
    "    #Send a GET request to the URL and parse the HTML content using BeautifulSoup\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content,'html.parser')\n",
    "\n",
    "    #Create empty lists to store the data\n",
    "    Restaurant_name = []\n",
    "    Cuisine = []\n",
    "    Location=[]\n",
    "    Ratings=[]\n",
    "    Image_URL=[]\n",
    "\n",
    "\n",
    "    #Find the relevant HTML elements and extract the data\n",
    "\n",
    "    for i in soup.find_all('a',class_=\"restnt-name ellipsis\"):\n",
    "        Restaurant_name.append(i.text)\n",
    "\n",
    "    for i in soup.find_all('div',class_=\"restnt-loc ellipsis\"):\n",
    "        Location.append(i.text)\n",
    "\n",
    "    for i in soup.find_all('img',class_=\"no-img\"):\n",
    "        Image_URL.append(i.get('data-src'))\n",
    "\n",
    "    for i in soup.find_all('span','double-line-ellipsis'):\n",
    "        Cuisine.append(i.text.split(\"|\")[1])\n",
    "\n",
    "    for i in soup.find_all('div',\"restnt-rating rating-4\"):\n",
    "        Ratings.append(i.text)\n",
    "\n",
    "    #Create a Pandas DataFrame from the lists        \n",
    "    df=pd.DataFrame({'Restaurant Name':Restaurant_name,'Cuisine':Cuisine,'Location':Location,'Ratings':Ratings,'Image_URL':Image_URL})\n",
    "\n",
    "    #Reset the index of the DataFrame and return it\n",
    "    df.index = range(1, len(df)+1)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7a69fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant Name</th>\n",
       "      <th>Cuisine</th>\n",
       "      <th>Location</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Image_URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>Chinese, North Indian</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jungle Jamboree</td>\n",
       "      <td>North Indian, Asian, Italian</td>\n",
       "      <td>3CS Mall,Lajpat Nagar - 3, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cafe Knosh</td>\n",
       "      <td>Italian, Continental</td>\n",
       "      <td>The Leela Ambience Convention Hotel,Shahdara, ...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>Chinese, North Indian</td>\n",
       "      <td>Pacific Mall,Tagore Garden, West Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Barbeque Company</td>\n",
       "      <td>North Indian, Chinese</td>\n",
       "      <td>Gardens Galleria,Sector 38A, Noida</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>India Grill</td>\n",
       "      <td>North Indian, Italian</td>\n",
       "      <td>Hilton Garden Inn,Saket, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Delhi Barbeque</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>Taurus Sarovar Portico,Mahipalpur, South Delhi</td>\n",
       "      <td>3.7</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Monarch - Bar Be Que Village</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>Indirapuram Habitat Centre,Indirapuram, Ghaziabad</td>\n",
       "      <td>3.8</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Indian Grill Room</td>\n",
       "      <td>North Indian, Mughlai</td>\n",
       "      <td>Suncity Business Tower,Golf Course Road, Gurgaon</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Restaurant Name                        Cuisine  \\\n",
       "1                   Castle Barbeque          Chinese, North Indian   \n",
       "2                   Jungle Jamboree   North Indian, Asian, Italian   \n",
       "3                        Cafe Knosh           Italian, Continental   \n",
       "4                   Castle Barbeque          Chinese, North Indian   \n",
       "5              The Barbeque Company          North Indian, Chinese   \n",
       "6                       India Grill          North Indian, Italian   \n",
       "7                    Delhi Barbeque                   North Indian   \n",
       "8  The Monarch - Bar Be Que Village                   North Indian   \n",
       "9                 Indian Grill Room          North Indian, Mughlai   \n",
       "\n",
       "                                            Location Ratings  \\\n",
       "1                     Connaught Place, Central Delhi       4   \n",
       "2             3CS Mall,Lajpat Nagar - 3, South Delhi     3.9   \n",
       "3  The Leela Ambience Convention Hotel,Shahdara, ...     4.3   \n",
       "4             Pacific Mall,Tagore Garden, West Delhi     3.9   \n",
       "5                 Gardens Galleria,Sector 38A, Noida     3.9   \n",
       "6               Hilton Garden Inn,Saket, South Delhi     3.9   \n",
       "7     Taurus Sarovar Portico,Mahipalpur, South Delhi     3.7   \n",
       "8  Indirapuram Habitat Centre,Indirapuram, Ghaziabad     3.8   \n",
       "9   Suncity Business Tower,Golf Course Road, Gurgaon     4.3   \n",
       "\n",
       "                                           Image_URL  \n",
       "1  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "2  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "3  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "4  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "5  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "6  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "7  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "8  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "9  https://im1.dineout.co.in/images/uploads/resta...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurant = dineout_restaurants('https://www.dineout.co.in/delhi-restaurants/buffet-special')\n",
    "restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b4849b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
